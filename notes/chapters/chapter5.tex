\chapter{Random Variables and Discrete Probability Distributions}  % 5

\section{Random Variables}  % 5.1
\subsection{Random Variables}  % 5.1.1
\dfn{random variable}{a numerical characteristic obtained from a random experiment. So, random variables are functions and follow all properties of mathematical functions.}

\subsection{Probability Distributions - pmf}  % 5.1.2
\dfn{probability mass function (pmf)}{The probability distributions of a random variable. In symbols, $p(x)=P(X=x)$}

\subsection{Properties}  % 5.1.3
Pmfs are valid probability distributions, so they follow the axioms of probability.
\begin{enumerate}
    \item $0\leq p_i\leq 1$. Each probability lies between 0 and 1.
    \item $\sum_i p_i(x)=1$. The sum of all probabilities is 1.
\end{enumerate}

\section{Expected Value and Variance}  % 5.2

\subsection{Expected Value}  % 5.2.1
\dfn{expected value}{the weighted average of each value of a discrete random variable $X$.}
\begin{equation} E(X)=\mu_X=\sum^m_{i=1} x_ip_i \end{equation}

\subsection{Rules of Expected Values}  % 5.2.2
\begin{enumerate}
    \item If $X$ is a random variable and $a$ and $b$ are fixed, then
          \[E(a+bX)=a+bE(X)\]
    \item If $X$ and $Y$ are random variables, then
          \[E(X+Y)=E(X)+E(Y)\]
    \item If $X$ is a random variable and $g$ is a function of $X$, then
          \[E(g(X))=\sum_{i=1}^m g(x_i)p_i\]
          \subitem \nt{}{$g(x)$ does not have to be linear}
\end{enumerate}

\subsection{Expectation, Variance, and Standard Deviation for a Discrete Variable}  % 5.2.3
Recall sample variance measures spread by taking the average of the squared differences between observations and their center
\begin{equation}
    s^2=\frac{\sum_{i=1}^{n}{(x_i-\bar{x})}^2}{n-1}
\end{equation}
\nt{}{Define the population variance of $X$ by $Var(X)$, $\sigma^2$, or $\sigma_X^2$.}
\begin{equation}
    Var(X)=\sigma^2 = \sigma_X^2
\end{equation}
The population variance is the expected squared difference between $X$ and $\mu_X$.
\begin{equation}
    Var(X)=E[{(X-\mu_X)}^2]=\sum{(x_i-\mu_X)}^2\cdot p_i
\end{equation}
Simplify.
\begin{equation}
    Var(X)=E[{(X-\mu_X)}^2]=E(X^2)-(E(X))^2
\end{equation}
Then, the standard deviation is the sqaure root of the variance
\begin{equation}
    \sigma_X=\sqrt{Var(X)}
\end{equation}

\subsection{Rules of Variance}  % 5.2.4
\begin{enumerate}
    \item if $X$ is a random variable and $a, b\in\bbR$ are fixed then
    \begin{equation}
        Var(a+bX)=b^2 Var(X)
    \end{equation}
    \nt{the variance does not rely on where hte center of the distribution is so a is not on RHS}
    \item if $X$ and $Y$ are independent random variables,
    \begin{equation}
        Var(X+Y)=Var(X)+Var(Y)\quad and\quad Var(X-Y)=Var(X)+Var(Y)
    \end{equation}
    addition rule for variances \\
    can be added for both addition and subtraction \\
\end{enumerate}

\section{Cumulative Distribution Function}  % 5.3
skip

\section{Binomial Random Variable}  % 5.4
\subsection{Binomial Experiment (BInS)}  % 5.4.1
Binary - are there only 2 options? \\
Independence - is each trial independent? \\
Number - is the number of trials a contant? \\
Success - is the probability of success a constant?
\subsection{Binomial Random Variables}  % 5.4.2
\dfn{Binomial Random Variable}{The \tit{binomial random variable} $X$ must come from a binomial experiment and maps every outcome to some $k\in\bbR$. $X$ has parameters $n$ (num trials) and $p$ (prob success).}

\subsection{PMF of Binomial Random }  % 5.4.3
If $X$ is a binomial random bariable with n trials and probility of a success p, then
\begin{equation}
    P(X=x)=\binom{n}{x}p^x(1-p)^{n-x},\ x=0,1,\ldots, n
\end{equation}

\subsection{Shapes of Binomial Distributions}
skewedness of a binom random variable depends on $p$. \\
$p<0.5$; distribution is skewed right \\
$p=0.5$; distribution is symmetric  \\
$0.5<p$; distribution is skewed left


\subsection{Binomial Distribution: Mean and Standard Deviation}
If $X\sim B(n,p)$, then
\begin{align}
    \EE(X)&=\mu_X-=np\\
    Var(X)&=np(1-p)\\
    \sigma_X&=\sqrt{np(1-p)}
\end{align}

\section{Poisson Random Variables}  % 5.5
\dfn{the \tit{poisson random variable} is used to count the number of events that happen during some interval or in some area.}

\ex{\# of people who enter the PMU per day; \# of radioactive particles emitted from some material in 1 minute}

\subsection{Poisson Experiment}  % 5.5.1
The poisson distribution also has an experiment, but it is more complex than  binomial
\begin{enumerate}
    \item the probability that an event occurs in some given interval is equal for any unit of equal size; the \tit{rate} is proportional to the size
    \item the number of events that occur within an interval i s independent of the number that occur in any other non-overlapping interval
    \item the probaiblity that more than one event occurs within a unit of measure is negligible for small units
\end{enumerate}

\subsection{Poisson Distribution}  % 5.5.2
only one param: $\lambda$ \\
Read $X\sim Poisson(\lambda)$ as ``$X$ is distributed as a Poisson random variable with parameter $\lambda$''
\begin{equation}
    p(x)=P(X=x)=\frac{e^{-\lambda}\lambda^x}{x!},\; x\in\N_0
\end{equation}

\subsection{Mean and Variance}  % 5.5.3
The mean and variance of the Poisson random variable are the same as $\lambda$. So,
\begin{align}
    \mu_X =\sigma\sq\\
    \sigma_X=\sqrt\lambda
\end{align}
$\lambda$ used instead of $\mu$ to maintain consistency with exponential distribution.\\
If units for \(\lambda\) were different than the units given in the problem, the problem can still be oslved due to the independence property (2) ofthe Poisson experiment.