\chapter{Random Variables and Discrete Probability Distributions}  % 5

\section{Random Variables}  % 5.1
\subsection{Random Variables}  % 5.1.1
\dfn{A \emph{random variable} is a numerical characteristic obtained from a random experiment. So, random variables are functions and follow all properties of mathematical functions.}

\subsection{Probability Distributions - pmf}  % 5.1.2
\dfn{The probability distributions of a random variable is called the \emph{probability mass function (pmf)}. In symbols, $p(x)=P(X=x)$}

\subsection{Properties}  % 5.1.3
Pmfs are valid probability distributions, so they follow the axioms of probability.
\begin{enumerate}
    \item $0\leq p_i\leq 1$. Each probability lies between 0 and 1.
    \item $\sum_i p_i(x)=1$. The sum of all probabilities is 1.
\end{enumerate}

\section{Expected Value and Variance}  % 5.2

\subsection{Expected Value}  % 5.2.1
\dfn{The \emph{expected value} of a discrete random variable $X$ is the weighted average of each value.}
\begin{equation} E(X)=\mu_X=\sum^m_{i=1} x_ip_i \end{equation}

\subsection{Rules of Expected Values}  % 5.2.2
\begin{enumerate}
    \item If $X$ is a random variable and $a$ and $b$ are fixed, then
          \[E(a+bX)=a+bE(X)\]
    \item If $X$ and $Y$ are random variables, then
          \[E(X+Y)=E(X)+E(Y)\]
    \item If $X$ is a random variable and $g$ is a function of $X$, then
          \[E(g(X))=\sum_{i=1}^m g(x_i)p_i\]
          \subitem \nt{$g(x)$ does not have to be linear}
\end{enumerate}

\subsection{Expectation, Variance, and Standard Deviation for a Discrete Variable}  % 5.2.3
Recall sample variance measures spread by taking the average of the squared differences between observations and their center
\begin{equation}
    s^2=\frac{\sum_{i=1}^{n}{(x_i-\bar{x})}^2}{n-1}
\end{equation}
\nt{Define the population variance of $X$ by $Var(X)$, $\sigma^2$, or $\sigma_X^2$.}
\begin{equation}
    Var(X)=\sigma^2 = \sigma_X^2
\end{equation}
The population variance is the expected squared difference between $X$ and $\mu_X$.
\begin{equation}
    Var(X)=E[{(X-\mu_X)}^2]=\sum{(x_i-\mu_X)}^2\cdot p_i
\end{equation}
Simplify.
\begin{equation}
    Var(X)=E[{(X-\mu_X)}^2]=E(X^2)-(E(X))^2
\end{equation}
Then, the standard deviation is the sqaure root of the variance
\begin{equation}
    \sigma_X=\sqrt{Var(X)}
\end{equation}

\subsection{Rules of Variance}  % 5.2.4
\begin{enumerate}
    \item if $X$ is a random variable and $a, b\in\RR$ are fixed then 
    \begin{equation}
        Var(a+bX)=b^2 Var(X)
    \end{equation} 
    \nt{the variance does not rely on where hte center of the distribution is so a is not on RHS}
    \item if $X$ and $Y$ are independent random variables, 
    \begin{equation}
        Var(X+Y)=Var(X)+Var(Y)\quad and\quad Var(X-Y)=Var(X)+Var(Y)
    \end{equation}
    addition rule for variances \\
    can be added for both addition and subtraction \\
    
\end{enumerate}

\section{Cumulative Distribution Function}  % 5.3
skip

\section{Binomial Random Variable}  % 5.4
\subsection{Binomial Experiment}  % 5.4.1
BINS -- Binary; Independent; Number of trials; constant probability of Success $P(S)=p$
\subsection{Binomial Random Variables}  % 5.4.2

\subsection{Mean and Variance}  % 5.4.3

\section{Poisson Random Variables}  % 5.5
\subsection{Poisson Experiment}  % 5.5.1
\subsection{Poisson Probabilities}  % 5.5.2
\subsection{Mean and Variance}  % 5.5.3
